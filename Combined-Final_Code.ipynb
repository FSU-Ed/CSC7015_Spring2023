{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./basicstats3_cleaned_enron.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess text-words message\n",
    "from nltk.stem import SnowballStemmer\n",
    "import re\n",
    "snow = nltk.stem.SnowballStemmer('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_words(sentence):\n",
    "    #lower case\n",
    "    sentence=sentence.lower() \n",
    "    #remove html\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    sentence = re.sub(cleanr, ' ', sentence)\n",
    "    #Normalizing URLs\n",
    "    cleanr = re.compile('(http|https)://[^\\s]*')\n",
    "    sentence = re.sub(cleanr, 'httpaddr', sentence)\n",
    "    #Removing Punctuation\n",
    "    sentence = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
    "    sentence = re.sub(r'[.|,|)|(|\\|/]',r' ',sentence)\n",
    "    #Normalize email address\n",
    "    cleanr = re.compile('[^\\s]+@[^\\s]+.com')\n",
    "    sentence = re.sub(cleanr, 'emailaddr', sentence)\n",
    "    #Normalize Numbers\n",
    "    cleanr = re.compile('[0-9]+')\n",
    "    sentence = re.sub(cleanr, 'number', sentence)\n",
    "    #Normalize money\n",
    "    cleanr = re.compile('[$]+')\n",
    "    sentence = re.sub(cleanr, 'dollar', sentence)\n",
    "    #Remove non-words\n",
    "    cleanr = re.compile('[^a-zA-Z0-9]')\n",
    "    sentence = re.sub(cleanr, ' ', sentence)\n",
    "    #Remove Subject\n",
    "    cleanr = re.compile('subject')\n",
    "    sentence = re.sub(cleanr, ' ', sentence)\n",
    "    #Removal of stop-words and Stemming\n",
    "    words = [snow.stem(word) for word in sentence.split() if word not in stopwords.words('english')]   \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = []\n",
    "counter = 0\n",
    "#Applying Preprocessing function\n",
    "for sentence in df['email']:\n",
    "    text_list.append(preprocessing_words(sentence))\n",
    "    #print(text_list[-1])\n",
    "    counter += 1\n",
    "    print('\\r{}/{}'.format(counter,len(df['email'])),end='')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_process = []\n",
    "counter = 0\n",
    "for row in text_list:\n",
    "    sequ = ''\n",
    "    for word in row:\n",
    "        sequ = sequ + ' ' + word\n",
    "    email_process.append(sequ)\n",
    "    counter += 1\n",
    "    print('\\r{}/{}'.format(counter,len(text_list)),end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def email_tokenization(data,features=500):\n",
    "  count_vect = CountVectorizer(max_features=features)\n",
    "  count_matrix = count_vect.fit_transform(data)\n",
    "  count_array = count_matrix.toarray()\n",
    "  tokens = pd.DataFrame(data=count_array,columns = count_vect.get_feature_names_out())\n",
    "  word = count_vect.vocabulary_\n",
    "\n",
    "  return (tokens, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, word = email_tokenization(email_process, features=2000)\n",
    "print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export process messages and labels\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = LabelBinarizer()\n",
    "lb.fit(df['label'])\n",
    "print(lb.classes_)\n",
    "y = lb.transform(df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Spam/Total')\n",
    "print('{}/{} '.format(int(sum(y)),len(y)))\n",
    "print('Spam proportion = {:0.2f}'.format(int(sum(y))/len(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_data = tokens\n",
    "export_data['label'] = y\n",
    "export_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_data.to_csv('token_mails_2000f_labeled.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Train\n",
    "df = pd.read_csv('./token_mails_2000f_labeled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,0:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset split\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test size is 20%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([len(X),len(X_train),len(X_test)],\n",
    "             index=['Total','Train','Test'],\n",
    "             columns=['Size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Spam/Total')\n",
    "print('{}/{} '.format(int(sum(y_train)),len(y_train)))\n",
    "print('Spam proportion = {:0.2f}'.format(int(sum(y_train))/len(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model selection\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross Validation\n",
    "#Learning Curve\n",
    "from sklearn.model_selection import learning_curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes_fraction = np.arange(0.1,0.85,0.05)\n",
    "train_sizes = np.array(train_sizes_fraction*len(y_train)).astype(int)\n",
    "print('Train sizes')\n",
    "print(train_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes, train_scores, valid_scores = learning_curve(gnb,X_train,y_train, train_sizes=train_sizes.astype(int), cv=5,scoring='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = pd.DataFrame([np.round(train_sizes),np.mean(train_scores,axis=1),np.mean(valid_scores,axis=1)],\n",
    "             index=['Training size','Training F1-score','CV F1-score']).T\n",
    "cv_results = cv_results.sort_values(by='CV F1-score', ascending=False)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.style.use('bmh');\n",
    "plt.plot(train_sizes,np.mean(train_scores,axis=1),'-o',label='Training score',);\n",
    "plt.plot(train_sizes,np.mean(valid_scores,axis=1),'-o',label='Cross-validation score');\n",
    "plt.ylim((0.5,1))\n",
    "plt.legend(loc=4,frameon=True);\n",
    "plt.xlabel('Training size');\n",
    "plt.ylabel('F1-Score');\n",
    "plt.title('Gaussian Naive Bayes');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb.fit(X_train,y_train)\n",
    "print('GNB score: ', f1_score(y_test,gnb.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature vs Performance\n",
    "iterat = np.arange(200,2001,200)\n",
    "best_models = np.zeros((len(iterat),4))\n",
    "counter = 0\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.style.use('bmh');\n",
    "\n",
    "for n_features in iterat:\n",
    "  #Cross-Validation for n features\n",
    "  print('\\r{}/{}'.format(counter+1,len(iterat)),end='')\n",
    "  (train_sizes, train_scores, valid_scores, \n",
    "   fit_time, score_time) = learning_curve(gnb,X_train.iloc[:,0:n_features],\n",
    "                                          y_train, train_sizes=train_sizes.astype(int),\n",
    "                                          cv=5,scoring='f1',return_times=True)\n",
    "   \n",
    "  cv_results = pd.DataFrame([np.round(train_sizes),np.mean(train_scores,axis=1),\n",
    "                             np.mean(valid_scores,axis=1),np.mean(fit_time,axis=1),\n",
    "                             np.mean(score_time,axis=1)],\n",
    "             index=['Training size','Training scores','CV scores','Fit time','Score time']).T\n",
    "  #Learning curves\n",
    "  plt.subplot(2,5,counter+1)\n",
    "  plt.plot(train_sizes,np.mean(train_scores,axis=1),'-o',label='Training score',);\n",
    "  plt.plot(train_sizes,np.mean(valid_scores,axis=1),'-o',label='Cross-validation score');\n",
    "  plt.ylim((0.5,1))\n",
    "  plt.legend(loc=4,frameon=True);\n",
    "  plt.xlabel('Training size');\n",
    "  plt.ylabel('F1-Score');\n",
    "  plt.title('Features: {}'.format(n_features));\n",
    "\n",
    "  #Extract best CV Score\n",
    "  best_models[counter,0] = n_features\n",
    "  best_models[counter,1] = cv_results[cv_results['CV scores']==cv_results['CV scores'].max()]['CV scores']\n",
    "  best_models[counter,2] = cv_results[cv_results['CV scores']==cv_results['CV scores'].max()]['Fit time']\n",
    "  best_models[counter,3] = cv_results[cv_results['CV scores']==cv_results['CV scores'].max()]['Score time']\n",
    "  counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models = pd.DataFrame(best_models,\n",
    "             columns=['Features','CV F1-scores','Fit time','Score time'])\n",
    "best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(best_models['Features'],best_models['CV F1-scores'],'-o');\n",
    "plt.xlabel('Features');\n",
    "plt.ylabel('CV F1-Score');\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(best_models['Features'],best_models['Fit time'],'-o');\n",
    "plt.xlabel('Features');\n",
    "plt.ylabel('Fit time');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best Model Validation\n",
    "#Training size for best model\n",
    "X_subtrain, X_cv, y_subtrain, y_cv = train_test_split(X_train,y_train,train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([len(X_train),len(X_subtrain),len(X_cv)],\n",
    "             index=['Total','Train','Test'],\n",
    "             columns=['Size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.fit(X_subtrain, y_subtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion amtrix and F1 score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('y-test size: {}'.format(len(y_test)))\n",
    "print('Ham: {}'.format(len(y_test)-sum(y_test)))\n",
    "print('Spam: {}'.format(sum(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = gnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, predictions, labels=gnb.classes_);\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=gnb.classes_);\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Metrics for best model')\n",
    "print('Accuracy: {:.2f}%'.format(accuracy_score(y_test,predictions)*100))\n",
    "print('Precision: {:.2f}%'.format(precision_score(y_test,predictions)*100))\n",
    "print('Recall: {:.2f}%'.format(recall_score(y_test,predictions)*100))\n",
    "print('F1 score: {:.2f}%'.format(f1_score(y_test,predictions)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,predictions,target_names=['ham','spam']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multilayer Perceptron Classifier\n",
    "# Training is probably the issue.  \n",
    "\n",
    "#Train the data\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#model=MLPClassifier()\n",
    "#this should be needed for the plots\n",
    "model=MLPClassifier(max_iter=500)\n",
    "\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction=model.predict(x_test)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "print(\"MLPClassifier\")\n",
    "print(\"Accuracy score: {}\". format(accuracy_score(y_test, prediction)) )\n",
    "print(\"Precision score: {}\". format(precision_score(y_test, prediction)) )\n",
    "print(\"Recall score: {}\". format(recall_score(y_test, prediction)))\n",
    "print(\"F1 score: {}\". format(f1_score(y_test, prediction)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross Validation\n",
    "#Learning Curve\n",
    "\n",
    "train_sizes_fraction = np.arange(0.1,0.85,0.05)\n",
    "train_sizes = np.array(train_sizes_fraction*len(y_train)).astype(int)\n",
    "print('Train sizes')\n",
    "print(train_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes, train_scores, valid_scores = learning_curve(model,x_train,y_train, train_sizes=train_sizes.astype(int), cv=5,scoring='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = pd.DataFrame([np.round(train_sizes),np.mean(train_scores,axis=1),np.mean(valid_scores,axis=1)],\n",
    "             index=['Training size','Training F1-score','CV F1-score']).T\n",
    "cv_results = cv_results.sort_values(by='CV F1-score', ascending=False)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.style.use('bmh');\n",
    "plt.plot(train_sizes,np.mean(train_scores,axis=1),'-o',label='Training score',);\n",
    "plt.plot(train_sizes,np.mean(valid_scores,axis=1),'-o',label='Cross-validation score');\n",
    "plt.ylim((0.5,1))\n",
    "plt.legend(loc=4,frameon=True);\n",
    "plt.xlabel('Training size');\n",
    "plt.ylabel('F1-Score');\n",
    "plt.title('Multilayer Perceptron Classifier');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train,y_train)\n",
    "print('MLPClassifier score: ', f1_score(y_test,model.predict(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature vs Performance\n",
    "iterate = np.arange(200,2001,200)\n",
    "best_models = np.zeros((len(iterate),4))\n",
    "counter = 0\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.style.use('bmh');\n",
    "\n",
    "for n_features in iterate:\n",
    "  #Cross-Validation for n features\n",
    "  print('\\r{}/{}'.format(counter+1,len(iterate)),end='')\n",
    "  (train_sizes, train_scores, valid_scores, \n",
    "   fit_time, score_time) = learning_curve(model,x_train.iloc[:,0:n_features],\n",
    "                                          y_train, train_sizes=train_sizes.astype(int),\n",
    "                                          cv=5,scoring='f1',return_times=True)\n",
    "   \n",
    "  cv_results = pd.DataFrame([np.round(train_sizes),np.mean(train_scores,axis=1),\n",
    "                             np.mean(valid_scores,axis=1),np.mean(fit_time,axis=1),\n",
    "                             np.mean(score_time,axis=1)],\n",
    "             index=['Training size','Training scores','CV scores','Fit time','Score time']).T\n",
    "  #Learning curves\n",
    "  plt.subplot(2,5,counter+1)\n",
    "  plt.plot(train_sizes,np.mean(train_scores,axis=1),'-o',label='Training score',);\n",
    "  plt.plot(train_sizes,np.mean(valid_scores,axis=1),'-o',label='Cross-validation score');\n",
    "  plt.ylim((0.5,1))\n",
    "  plt.legend(loc=4,frameon=True);\n",
    "  plt.xlabel('Training size');\n",
    "  plt.ylabel('F1-Score');\n",
    "  plt.title('Features: {}'.format(n_features));\n",
    "\n",
    "  #Extract best CV Score\n",
    "  best_models[counter,0] = n_features\n",
    "  best_models[counter,1] = cv_results[cv_results['CV scores']==cv_results['CV scores'].max()]['CV scores']\n",
    "  best_models[counter,2] = cv_results[cv_results['CV scores']==cv_results['CV scores'].max()]['Fit time']\n",
    "  best_models[counter,3] = cv_results[cv_results['CV scores']==cv_results['CV scores'].max()]['Score time']\n",
    "  counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models = pd.DataFrame(best_models,\n",
    "             columns=['Features','CV F1-scores','Fit time','Score time'])\n",
    "best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(best_models['Features'],best_models['CV F1-scores'],'-o');\n",
    "plt.xlabel('Features');\n",
    "plt.ylabel('CV F1-Score');\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(best_models['Features'],best_models['Fit time'],'-o');\n",
    "plt.xlabel('Features');\n",
    "plt.ylabel('Fit time');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best Model Validation\n",
    "#Training size for best model\n",
    "x_subtrain, x_cv, y_subtrain, y_cv = train_test_split(x_train,y_train,train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([len(x_train),len(x_subtrain),len(x_cv)],\n",
    "             index=['Total','Train','Test'],\n",
    "             columns=['Size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_subtrain, y_subtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion matrix and F1 score\n",
    "print('y-test size: {}'.format(len(y_test)))\n",
    "print('Ham: {}'.format(len(y_test)-sum(y_test)))\n",
    "print('Spam: {}'.format(sum(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, predictions, labels=model.classes_);\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_);\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Metrics for best model')\n",
    "print('Accuracy: {:.2f}%'.format(accuracy_score(y_test,predictions)*100))\n",
    "print('Precision: {:.2f}%'.format(precision_score(y_test,predictions)*100))\n",
    "print('Recall: {:.2f}%'.format(recall_score(y_test,predictions)*100))\n",
    "print('F1 score: {:.2f}%'.format(f1_score(y_test,predictions)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,predictions,target_names=['ham','spam']))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
